{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0,1, n_steps)\n",
    "    series = 0.5 * np.sin( (time-offsets1) * (freq1*10 + 10))\n",
    "    series += 0.2 * np.sin( (time-offsets2) * (freq2*20 + 20))\n",
    "    series += 0.1 * (np.random.rand(batch_size,n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps+1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021933144"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X_valid[:, -1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 1s 1ms/step - loss: 0.1051 - val_loss: 0.0569\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 0s 547us/step - loss: 0.0404 - val_loss: 0.0287\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 0s 506us/step - loss: 0.0218 - val_loss: 0.0168\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 0s 504us/step - loss: 0.0140 - val_loss: 0.0117\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 0s 513us/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 0s 546us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 0s 558us/step - loss: 0.0082 - val_loss: 0.0075\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 0s 481us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 0s 431us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 0s 472us/step - loss: 0.0062 - val_loss: 0.0059\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 0s 447us/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 0s 532us/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 0s 555us/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 0s 515us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 0s 506us/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 0s 512us/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 0s 489us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 0s 484us/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 0s 549us/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 0s 530us/step - loss: 0.0041 - val_loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50,]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                   validation_data = (X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 2s 6ms/step - loss: 0.2263 - val_loss: 0.1519\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.1051 - val_loss: 0.0766\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0667 - val_loss: 0.0596\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0549 - val_loss: 0.0506\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0469 - val_loss: 0.0437\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0409 - val_loss: 0.0384\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0361 - val_loss: 0.0342\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0323 - val_loss: 0.0308\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.0280\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0266 - val_loss: 0.0256\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0244 - val_loss: 0.0236\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0225 - val_loss: 0.0219\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0209 - val_loss: 0.0204\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0195 - val_loss: 0.0191\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0183 - val_loss: 0.0179\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0172 - val_loss: 0.0169\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0162 - val_loss: 0.0160\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0154 - val_loss: 0.0152\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0146 - val_loss: 0.0145\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0140 - val_loss: 0.0139\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None,1])\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                   validation_data = (X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.013887662440538406"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 6s 19ms/step - loss: 0.0167 - val_loss: 0.0043\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0038 - val_loss: 0.0033\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0029 - val_loss: 0.0034\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0028 - val_loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None,1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 4s 13ms/step - loss: 0.0208 - val_loss: 0.0047\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0043 - val_loss: 0.0036\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0029 - val_loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "\n",
    "series = generate_time_series(1, n_steps+10)\n",
    "X_new, Y_new = series[ :, :n_steps], series[:, n_steps:]\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X[:, step_ahead:])[:,np.newaxis,:]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "    \n",
    "Y_pred = X[:, n_steps:]\n",
    "Y_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_valid\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X)[:, np.newaxis, :]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "\n",
    "Y_pred = X[:, n_steps:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023283344"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(keras.metrics.mean_squared_error(Y_valid, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25697407"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_naive_pred = np.tile(X_valid[:, -1], 10) # take the last time step value, and repeat it 10 times\n",
    "np.mean(keras.metrics.mean_squared_error(Y_valid, Y_naive_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 4s 13ms/step - loss: 0.0669 - val_loss: 0.0317\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0265 - val_loss: 0.0200\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0183 - val_loss: 0.0160\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0155 - val_loss: 0.0144\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0139 - val_loss: 0.0118\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0128 - val_loss: 0.0112\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0122 - val_loss: 0.0110\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0095 - val_loss: 0.0107\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0092 - val_loss: 0.0089\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0094 - val_loss: 0.0111\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0094 - val_loss: 0.0085\n"
     ]
    }
   ],
   "source": [
    "# With sequential model\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 타깃마다 10차원 벡터를 담은 타깃 시퀀스를 준비\n",
    "\n",
    "Y = np.empty((10000, n_steps, 10))\n",
    "\n",
    "for step_ahead in range(1, 10+1):\n",
    "    Y[:, :, step_ahead -1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모든 타임 스텝에서 출력을 Dense 층에 적용해야한다. 케라스의 TimeDistributed 층을 이용하면 가능하다. 각 타임 스텝을 별개의 샘플처럼 다루도록 입력의 크기를 바꾸어 이를 효과적으로 수행한다. \n",
    "* 즉 입력을 (batch_size, time_step, input_dimension)에서 (batch_size * time_step, input_dimension)로 바꿔준다\n",
    "* 그다음 Dense층에 적용한다.\n",
    "* 마지막으로 출력 크기를 시퀀스로 돌려준다. 즉 출력을 (batch_size * time_step, output_dimension)에서 (batch_size, time_step, output_dimension)로 바꿔준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 4s 14ms/step - loss: 0.0472 - last_time_step_mse: 0.0360 - val_loss: 0.0362 - val_last_time_step_mse: 0.0218\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0351 - last_time_step_mse: 0.0228 - val_loss: 0.0322 - val_last_time_step_mse: 0.0210\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0301 - last_time_step_mse: 0.0185 - val_loss: 0.0314 - val_last_time_step_mse: 0.0213\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0276 - last_time_step_mse: 0.0159 - val_loss: 0.0244 - val_last_time_step_mse: 0.0125\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0252 - last_time_step_mse: 0.0132 - val_loss: 0.0239 - val_last_time_step_mse: 0.0120\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0245 - last_time_step_mse: 0.0122 - val_loss: 0.0245 - val_last_time_step_mse: 0.0127\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0237 - last_time_step_mse: 0.0113 - val_loss: 0.0215 - val_last_time_step_mse: 0.0102\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0230 - last_time_step_mse: 0.0107 - val_loss: 0.0215 - val_last_time_step_mse: 0.0093\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0225 - last_time_step_mse: 0.0105 - val_loss: 0.0214 - val_last_time_step_mse: 0.0094\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0216 - last_time_step_mse: 0.0094 - val_loss: 0.0204 - val_last_time_step_mse: 0.0086\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0221 - last_time_step_mse: 0.0100 - val_loss: 0.0215 - val_last_time_step_mse: 0.0102\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0209 - last_time_step_mse: 0.0089 - val_loss: 0.0197 - val_last_time_step_mse: 0.0080\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0209 - last_time_step_mse: 0.0091 - val_loss: 0.0192 - val_last_time_step_mse: 0.0072\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0213 - last_time_step_mse: 0.0097 - val_loss: 0.0199 - val_last_time_step_mse: 0.0085\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0197 - last_time_step_mse: 0.0079 - val_loss: 0.0186 - val_last_time_step_mse: 0.0075\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0194 - last_time_step_mse: 0.0076 - val_loss: 0.0186 - val_last_time_step_mse: 0.0076\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0192 - last_time_step_mse: 0.0074 - val_loss: 0.0193 - val_last_time_step_mse: 0.0088\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0188 - last_time_step_mse: 0.0071 - val_loss: 0.0231 - val_last_time_step_mse: 0.0127\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0195 - last_time_step_mse: 0.0080 - val_loss: 0.0174 - val_last_time_step_mse: 0.0059\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0182 - last_time_step_mse: 0.0066 - val_loss: 0.0203 - val_last_time_step_mse: 0.0092\n"
     ]
    }
   ],
   "source": [
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.01), \n",
    "              metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Long Sequences\n",
    "\n",
    "* 긴 sequnce에서 Relu 사용시 출력 폭주의 가능성 있음. (gradients 도 폭주 가능)\n",
    "* Gradient Clipping, smaller learning rate 사용해서 해결 가능\n",
    "\n",
    "* BatchNormalization은 RNN과 효율적으로 사용할 수 없다. (스텝 사이에는 사용불가, 순환층 사이에만 적용 가능) 순환층 이전에 주로 사용한다.\n",
    "\n",
    "* RNN에선 보통 layer normalization을 이용한다. 이는 배치 차원에 대해 정규화하는 대신 특성 차원에 대해 정규화한다. 장점은 샘플에 독립적으로 타임 스텝마다 필요한 통계를 계산할 수 있다. 이는 훈련과 테스트에서 동일한 방식으로 작동한다는것을 의미.(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,\n",
    "                                                          activation=None)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras.layers.Layer 상속받음. \n",
    "* 유닛 개수와 활성화 함수를 매개변수로 받고, state_size와 output_size 속성을 설정한 다음 활성화 함수 없이 SimpleRNNCell을 만든다. (선형 연산후와 활성화 함수 전에 층 정규화를 수행하기 위해서)\n",
    "* call() 메서드에선 간단한 RNN셀을 적용하여 현재 입력과 이전 은닉 상태의 선형 조합을 계산한다.\n",
    "* 그다음 층 정규화와 활성화함수를 차례데로 적용, 출력과 새로운 은닉을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 8s 28ms/step - loss: 0.1625 - last_time_step_mse: 0.1576 - val_loss: 0.0762 - val_last_time_step_mse: 0.0732\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0663 - last_time_step_mse: 0.0578 - val_loss: 0.0589 - val_last_time_step_mse: 0.0477\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0551 - last_time_step_mse: 0.0431 - val_loss: 0.0511 - val_last_time_step_mse: 0.0372\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.0487 - last_time_step_mse: 0.0347 - val_loss: 0.0457 - val_last_time_step_mse: 0.0310\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.0439 - last_time_step_mse: 0.0291 - val_loss: 0.0425 - val_last_time_step_mse: 0.0284\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.0399 - last_time_step_mse: 0.0249 - val_loss: 0.0371 - val_last_time_step_mse: 0.0211\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0358 - last_time_step_mse: 0.0208 - val_loss: 0.0339 - val_last_time_step_mse: 0.0188\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0331 - last_time_step_mse: 0.0184 - val_loss: 0.0326 - val_last_time_step_mse: 0.0181\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.0315 - last_time_step_mse: 0.0172 - val_loss: 0.0307 - val_last_time_step_mse: 0.0163\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0302 - last_time_step_mse: 0.0163 - val_loss: 0.0294 - val_last_time_step_mse: 0.0151\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0294 - last_time_step_mse: 0.0156 - val_loss: 0.0286 - val_last_time_step_mse: 0.0150\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0289 - last_time_step_mse: 0.0155 - val_loss: 0.0291 - val_last_time_step_mse: 0.0169\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0279 - last_time_step_mse: 0.0144 - val_loss: 0.0272 - val_last_time_step_mse: 0.0134\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0274 - last_time_step_mse: 0.0140 - val_loss: 0.0269 - val_last_time_step_mse: 0.0137\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0269 - last_time_step_mse: 0.0136 - val_loss: 0.0264 - val_last_time_step_mse: 0.0127\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.0266 - last_time_step_mse: 0.0135 - val_loss: 0.0261 - val_last_time_step_mse: 0.0127\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0261 - last_time_step_mse: 0.0129 - val_loss: 0.0258 - val_last_time_step_mse: 0.0127\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0258 - last_time_step_mse: 0.0128 - val_loss: 0.0256 - val_last_time_step_mse: 0.0128\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0255 - last_time_step_mse: 0.0126 - val_loss: 0.0250 - val_last_time_step_mse: 0.0118\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 6s 25ms/step - loss: 0.0251 - last_time_step_mse: 0.0121 - val_loss: 0.0247 - val_last_time_step_mse: 0.0113\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
    "                     input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer_normalization 말고 dropout을 단계 사이에 적용할수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "* RNN을 오래 돌리면 첫번째 입력의 흔적은 남지 않는 상황이 발생한다.\n",
    "* 이런 문제를 해결하기 위한 long short-term memory 에 대해 알아보자\n",
    "* 사용법은 간단하게 SimpleRNN층 대신 LSTM층 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lstm](https://github.com/stop0729/Hands_on_machine_learning/tree/main/ch15/lstm.png)\n",
    "\n",
    "* 네트워크가 장기상태에 저장할것, 버릴것, 읽어들일것을 학습한다.\n",
    "* c(t-1)가 왼쪽에서 우측으로 가면서 forget gate에서 버릴것은 버리고, addition operation을 통하여 input gate로 들어온것을 더한다.\n",
    "* operation이 끝난 c(t)는 복사되어 하나는 그냥 우측으로 빠져나가고, 하나는 tanh 함수를 거치고 output gate에 의해 필터링 된다. 이것은 단기 상태 h(t)를 만든다. h(t)는 y(t)와 같다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell에 들어오는 것은 input vector x(t)와 이전 단기 상태h(t)가 있다. 이들은 4가지 fully connected layer로 쪼개진다.\n",
    "\n",
    "* main layer 는 g(t)를 출력하는 층이다. 현랙입력 x(t)와이전 단기 상태 h(t-1)를 분석하는 역활을 한다. 기본 셀에서는 이 층 외에는 다른것은 없고 바로 h(t)와 y(t)로 출력된다. 그러나 lstm에서는 이 층의 출력이 바로 나가지 않고 장기상태에 가장 중요한 부분이 저장된다.\n",
    "* 나머지 3개의 층은 gate controller 이다. 이들은 로지스틱 활성화 함수를 거쳐 0과 1사이 범위를 가진다. 이들의 출력은 원소별 곱셈 연산으로 주입되며, 0을 출력하면 게이트를 닫고 1을 출력하면 게이트를 연다.\n",
    "\n",
    "각 게이트에 사용되는 공식은 이러하다. (나중에 보자 ..)\n",
    "\n",
    "![lstm_equation](./lstm_equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GRU](./GRU.png)\n",
    "\n",
    "LSTM 셀의 간소화된 버전이다. LSTM과 성능이 비슷하다.\n",
    "* c(t)와 h(t)가 합쳐졌다.\n",
    "* gate controller가 z(t) 하나로 간소화 됐다. controller가 1을 출력하면 삭제게이트가 열리고, 입력 게이트가 닫힌다. 0을 출력하면 그 반대다. 다시 말하자면 메모리가 저장될떄마다, 저장될 위치가 먼저 비워진다.\n",
    "* output gate가 없다. 전체 상태가 변함없이 매 타입스텝마다 출력된다. 그러나 새로운controller r(t)가 있어서 어느 부분이 노출할지 정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GRU_equation](./GRU_equation.png)\n",
    "\n",
    "(이 게이트 공식들 역시 나중에 보자..)\n",
    "\n",
    "이 방법들 역시 sequence가 100번보다 많아지면 문제가 생긴다. 이를 해결하는 방법은 input sequences를 1D convolutional layers 를 이용하여 줄이는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 1D convolutional layers to process sequences\n",
    "\n",
    "10개 커널의 1D convolution을 사용하면 긴 sequences 들은 한개의 10차원 sequeces로 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 8s 19ms/step - loss: 0.0681 - last_time_step_mse: 0.0601 - val_loss: 0.0477 - val_last_time_step_mse: 0.0396\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0414 - last_time_step_mse: 0.0340 - val_loss: 0.0367 - val_last_time_step_mse: 0.0285\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0338 - last_time_step_mse: 0.0257 - val_loss: 0.0307 - val_last_time_step_mse: 0.0218\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0282 - last_time_step_mse: 0.0184 - val_loss: 0.0259 - val_last_time_step_mse: 0.0152\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0249 - last_time_step_mse: 0.0143 - val_loss: 0.0246 - val_last_time_step_mse: 0.0141\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0234 - last_time_step_mse: 0.0125 - val_loss: 0.0227 - val_last_time_step_mse: 0.0115\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0226 - last_time_step_mse: 0.0117 - val_loss: 0.0225 - val_last_time_step_mse: 0.0116\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0220 - last_time_step_mse: 0.0111 - val_loss: 0.0216 - val_last_time_step_mse: 0.0105\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0216 - last_time_step_mse: 0.0108 - val_loss: 0.0217 - val_last_time_step_mse: 0.0109\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0213 - last_time_step_mse: 0.0106 - val_loss: 0.0210 - val_last_time_step_mse: 0.0102\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0210 - last_time_step_mse: 0.0102 - val_loss: 0.0208 - val_last_time_step_mse: 0.0100\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0208 - last_time_step_mse: 0.0102 - val_loss: 0.0208 - val_last_time_step_mse: 0.0102\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0205 - last_time_step_mse: 0.0098 - val_loss: 0.0206 - val_last_time_step_mse: 0.0101\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 3s 14ms/step - loss: 0.0204 - last_time_step_mse: 0.0099 - val_loss: 0.0204 - val_last_time_step_mse: 0.0099\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0202 - last_time_step_mse: 0.0097 - val_loss: 0.0199 - val_last_time_step_mse: 0.0093\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0200 - last_time_step_mse: 0.0097 - val_loss: 0.0201 - val_last_time_step_mse: 0.0095\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0196 - last_time_step_mse: 0.0093 - val_loss: 0.0197 - val_last_time_step_mse: 0.0091\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 3s 14ms/step - loss: 0.0194 - last_time_step_mse: 0.0090 - val_loss: 0.0192 - val_last_time_step_mse: 0.0086\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0190 - last_time_step_mse: 0.0087 - val_loss: 0.0188 - val_last_time_step_mse: 0.0084\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0186 - last_time_step_mse: 0.0083 - val_loss: 0.0184 - val_last_time_step_mse: 0.0080\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
    "                        input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train[:, 3::2], epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid[:, 3::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet\n",
    "\n",
    "![Wavenet](./Wavenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 네트워크는 층마다 dilation rate (각 뉴런의 입력이 떨어져 있는 간격)을 두배로 늘리는 1D 합성곱층을 계속 쌓는다. 그림에서 보듯 input은 전부 들어가고 그 위층은 2개의 타임 스텝 간격, 그 위층은 4개의 타임 스텝 간격으로 들어간다.\n",
    "이런식으로 하위층은 단기 패턴을 학습하고, 상위층은 장기 패턴을 학습한다. \n",
    "\n",
    "긴 sequence를 효율적으로 처리할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 3s 9ms/step - loss: 0.0675 - last_time_step_mse: 0.0552 - val_loss: 0.0347 - val_last_time_step_mse: 0.0206\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0319 - last_time_step_mse: 0.0186 - val_loss: 0.0298 - val_last_time_step_mse: 0.0172\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0287 - last_time_step_mse: 0.0159 - val_loss: 0.0283 - val_last_time_step_mse: 0.0164\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0267 - last_time_step_mse: 0.0140 - val_loss: 0.0258 - val_last_time_step_mse: 0.0136\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0253 - last_time_step_mse: 0.0129 - val_loss: 0.0246 - val_last_time_step_mse: 0.0121\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0242 - last_time_step_mse: 0.0118 - val_loss: 0.0234 - val_last_time_step_mse: 0.0109\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0235 - last_time_step_mse: 0.0113 - val_loss: 0.0228 - val_last_time_step_mse: 0.0104\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0226 - last_time_step_mse: 0.0103 - val_loss: 0.0221 - val_last_time_step_mse: 0.0097\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0218 - last_time_step_mse: 0.0096 - val_loss: 0.0225 - val_last_time_step_mse: 0.0106\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.0214 - last_time_step_mse: 0.0091 - val_loss: 0.0208 - val_last_time_step_mse: 0.0085\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.0207 - last_time_step_mse: 0.0084 - val_loss: 0.0206 - val_last_time_step_mse: 0.0084\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.0203 - last_time_step_mse: 0.0080 - val_loss: 0.0198 - val_last_time_step_mse: 0.0075\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0198 - last_time_step_mse: 0.0076 - val_loss: 0.0197 - val_last_time_step_mse: 0.0076\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0195 - last_time_step_mse: 0.0074 - val_loss: 0.0196 - val_last_time_step_mse: 0.0074\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0192 - last_time_step_mse: 0.0071 - val_loss: 0.0192 - val_last_time_step_mse: 0.0072\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0191 - last_time_step_mse: 0.0070 - val_loss: 0.0189 - val_last_time_step_mse: 0.0069\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0188 - last_time_step_mse: 0.0068 - val_loss: 0.0186 - val_last_time_step_mse: 0.0069\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0185 - last_time_step_mse: 0.0066 - val_loss: 0.0184 - val_last_time_step_mse: 0.0065\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0184 - last_time_step_mse: 0.0065 - val_loss: 0.0183 - val_last_time_step_mse: 0.0066\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0182 - last_time_step_mse: 0.0065 - val_loss: 0.0180 - val_last_time_step_mse: 0.0061\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "for rate in (1, 2, 4, 8) * 2:\n",
    "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
    "                                  activation=\"relu\", dilation_rate=rate))\n",
    "    \n",
    "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                   validation_data = (X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* padding = 'casual'을 사용하면 미래의 sequence를 훔쳐보지 않는다. 'valid'사용과 같다.\n",
    "* (1, 2, 4, 8) * 2을 써서 Conv1D 층을 두번 쌓는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
